{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a28c6e-9f06-44fa-906c-a1a950c715b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e3f49c-cb38-4de4-a409-ed3ead07f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitInputLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 emb_dim:int=384,\n",
    "                 num_patch_row:int=2,\n",
    "                 image_size:int=32\n",
    "                ):\n",
    "        \"\"\"\n",
    "        in_channels : 入力画像のチャンネル数\n",
    "        emb_dim : 埋め込み後のベクトルの長さ\n",
    "        num_patch : 高さ方向のパッチの数\n",
    "        image_size : 入力画像の1辺の長さ，入力画像の高さと幅は同じであると仮定\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_patch_row = num_patch_row\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.num_patch = self.num_patch_row ** 2\n",
    "\n",
    "        # パッチの大きさ\n",
    "        # 例 : 入力画像の１辺の長さが32， patch_size_row=2の場合, patch_size = 16\n",
    "        self.patch_size = int(self.image_size // self.num_patch_row)\n",
    "\n",
    "        # 入寮画像のパッチへの分割 & パッチの埋め込みを一気に行う層 => kernelを学習することを埋め込みと呼んでいる？\n",
    "        self.patch_emb_layer = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.emb_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "\n",
    "        # クラストークン\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, emb_dim)\n",
    "        )\n",
    "\n",
    "        # 位置埋め込み\n",
    "        self.pos_emb = nn.Parameter(\n",
    "            torch.randn(1, self.num_patch+1, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            x : 入力画像.shape => (B, C, H, W)\n",
    "                B: batch_size, C: num_channels, H: height, W: width\n",
    "        返り値:\n",
    "            z_0: ViTへの入力.shape => (B, N, D)\n",
    "                B: batch_size, N: num_token, D: emb_dim\n",
    "        \"\"\"\n",
    "\n",
    "        # バッチの埋め込み & flatten\n",
    "        ## (B, C, H, W) -> (B, D, H/P, W/P)\n",
    "        z_0 = self.patch_emb_layer(x)\n",
    "        ## (B, D, H/P, W/P) -> (B, D, Np)\n",
    "        z_0 = z_0.flatten(2)\n",
    "        ## 軸の入れ替え (B, D, Np) -> (B, Np, D)\n",
    "        z_0 = z_0.transpose(1,2)\n",
    "\n",
    "        # バッチの埋め込みの先頭に暮らすトークンを結合\n",
    "        ## (B, Np, D) -> (B, N, D)\n",
    "        z_0 = torch.cat(\n",
    "            [self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1\n",
    "        )\n",
    "\n",
    "        # 位置埋め込みの加算\n",
    "        ## (B, N, D) -> (B, N, D)\n",
    "        z_0 = z_0 + self.pos_emb\n",
    "\n",
    "        return z_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fcbe97-8505-40a4-8beb-935877ebb110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "batch_size, channel, height, width = 2, 3, 32, 32\n",
    "x = torch.randn(batch_size, channel, height, width)\n",
    "input_layer = VitInputLayer()\n",
    "z_0 = input_layer(x)\n",
    "print(z_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4776e6b-73bc-4dcb-b26b-85b65202643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_dim:int=384,\n",
    "                 head:int=3,\n",
    "                 dropout:float=0.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            emb_dim : 埋め込み後のベクトル長\n",
    "            head : ヘッドの数\n",
    "            dropout : ドロップアウト率\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.emb_dim = emb_dim\n",
    "        self.head_dim = emb_dim // self.head\n",
    "        self.sqrt_dh = self.head_dim**0.5 # D_hの二乗根, qk^Tを割るための係数\n",
    "        \n",
    "        # 入力をq,k,vに埋め込むための線形層\n",
    "        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "\n",
    "        # ドロップアウト層\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # MHSAの結果を出力に埋め込むための線形層\n",
    "        self.w_o = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, z:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            z: MHSAのへの入力．shape => (B, N, D)\n",
    "                B: batch_size, N: トークン数, D: 埋め込みベクトル長\n",
    "        返り値:\n",
    "            out: MHSAの出力. shape => (B, N, D)\n",
    "        \"\"\"\n",
    "        batch_size, num_patch, _ = z.shape\n",
    "\n",
    "        # 埋め込み\n",
    "        q = self.w_q(z)\n",
    "        k = self.w_k(z)\n",
    "        v = self.w_v(z)\n",
    "\n",
    "        # q, k, vをヘッドに分ける\n",
    "        ## (B, N, D) -> (B, N, h, D/h)\n",
    "        q = q.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        k = k.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        v = v.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "\n",
    "        ## Self-Attentionができるように\n",
    "        ## (B, N, h, D/h) -> (B, h, N, D/h)\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # 内積\n",
    "        ## (B, h, N, D/h) -> (B, h, D/h, N)\n",
    "        k_T = k.transpose(2,3)\n",
    "        ## (B, h, N, D/h) x (B, h, D/h, N) -> (B, h, N, N)\n",
    "        dots = (q @ k_T) / self.sqrt_dh\n",
    "        ## 列方向にソフトマックス\n",
    "        attn = F.softmax(dots, dim=-1)\n",
    "        ## ドロップアウト\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        #　加重和\n",
    "        ## (B, h, N, N) x (B, h, N, D/h) -> (B, h, N, D/h)\n",
    "        out = attn @ v\n",
    "        ## (B, h, N, D/h) -> (B, N, h, D/h)\n",
    "        out = out.transpose(1,2)\n",
    "        ## (B, N, h, D/h) -> (B, N, D)\n",
    "        out = out.reshape(batch_size, num_patch, self.emb_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60eb579f-a507-4d99-aed8-937b58779b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "mhsa = MultiHeadSelfAttention()\n",
    "out = mhsa(z_0)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b39181fb-9b3f-4c6e-b641-8ff62185680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_dim:int=384,\n",
    "                 head:int=8,\n",
    "                 hidden_dim:int=384*4,\n",
    "                 dropout:float=0.\n",
    "                ):\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            emb_dim: 埋め込み後のベクトルの長さ\n",
    "            head: ヘッドの数\n",
    "            hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ\n",
    "            dropout: ドロップアウト率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.msa = MultiHeadSelfAttention(\n",
    "            emb_dim=emb_dim,\n",
    "            head=head,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, z:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            z: Encoder Blockへの入力. shape=(B, N, D)\n",
    "                B: batch_size, N: トークン数, D: 埋め込みベクトルの長さ\n",
    "        返り値:\n",
    "            out: Encoder Blockの出力. shape=(B, N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.msa(self.ln1(z)) + z\n",
    "\n",
    "        out = self.mlp(self.ln2(z)) * out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b9338c2-7ae2-4e40-b3ec-789b5c93bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "vit_enc = VitEncoderBlock()\n",
    "z_1 = vit_enc(z_0)\n",
    "print(z_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ebd2dfe-090b-440e-9ca7-09f419b0a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channel:int=3,\n",
    "                 num_classes:int=10,\n",
    "                 emb_dim:int=384,\n",
    "                 num_patch_row:int=2,\n",
    "                 image_size:int=32,\n",
    "                 num_blocks:int=7,\n",
    "                 head:int=8,\n",
    "                 hidden_dim:int=384*4,\n",
    "                 dropout=0.\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = VitInputLayer(\n",
    "            in_channel,\n",
    "            emb_dim,\n",
    "            num_patch_row,\n",
    "            image_size\n",
    "        )\n",
    "\n",
    "        # Encoder Blockの多段\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            VitEncoderBlock(\n",
    "                emb_dim=emb_dim,\n",
    "                head=head,\n",
    "                hidden_dim=hidden_dim,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        # MLP Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            x: Vitへの入力画像. shape = (B, C, H, W)\n",
    "        返り値\n",
    "            out: Vitの出力. shape = (B, M)\n",
    "            M: クラス数\n",
    "        \"\"\"\n",
    "        # InputLayer\n",
    "        ## (B, C, H, W) -> (B, N, D)\n",
    "        out = self.input_layer(x)\n",
    "        # Encoder\n",
    "        ## (B, N, D) -> (B, N, D)\n",
    "        out = self.encoder(out)\n",
    "        # クラストークンだけ取り出す\n",
    "        ## (B, N, D) -> (B, D)\n",
    "        cls_token = out[:, 0]\n",
    "        # MLP Head\n",
    "        ## (B, D) -> (B, M)\n",
    "        pred = self.mlp_head(cls_token)\n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896febe-fa82-4be9-b444-5b7dd95b399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch_size, channel, height, width = 2, 3, 32, 32\n",
    "x = torch.randn(batch_size, channel, heightm width)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
